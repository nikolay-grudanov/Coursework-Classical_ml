{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook locally, you can create a dedicated ipykernel named \"ml\" so that the notebook uses the intended kernel environment:\n",
    "\n",
    "python -m ipykernel install --user --name=ml --display-name \"ml\"\n",
    "\n",
    "Also ensure the DATA_PATH environment variable is set (or a .env file is present) to point to your data file. The notebook will read DATA_PATH from the environment and fall back to a default path if not provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for Drug Effectiveness Prediction\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on the chemical compounds dataset to understand the data structure, identify patterns, and check for potential data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db54a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# NOTE: This is a lightweight textual replacement of the hard-coded data path and kernel metadata.\n",
    "# Replace the explicit data path with environment-aware loading and update kernel name to 'ml'.\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Update kernel metadata manually when using Jupyter: create kernel named 'ml' with\n",
    "# python -m ipykernel install --user --name=ml --display-name \"ml\"\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "DATA_PATH = os.environ.get('DATA_PATH', '/home/gna/workspase/education/MEPHI/Coursework-Classical_ml/data/data.xlsx')\n",
    "print(f\"Loading data from {DATA_PATH}\")\n",
    "\n",
    "data = pd.read_excel(DATA_PATH)\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)[:10]}...\")  # Show first 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary column\n",
    "if 'Unnamed: 0' in data.columns:\n",
    "    data = data.drop(columns=['Unnamed: 0'])\n",
    "    print(\"Dropped 'Unnamed: 0' column\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape after cleaning: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = data.isnull().sum()\n",
    "missing_percent = 100 * missing_data / len(data)\n",
    "missing_table = pd.DataFrame({\n",
    "    'Missing Values': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "missing_table = missing_table[missing_table['Missing Values'] > 0].sort_values('Percentage', ascending=False)\n",
    "print(\"Missing values in the dataset:\")\n",
    "print(missing_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for target variables\n",
    "target_vars = ['IC50, mM', 'CC50, mM', 'SI']\n",
    "target_stats = data[target_vars].describe()\n",
    "print(\"Basic statistics for target variables:\")\n",
    "print(target_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for target variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, target in enumerate(target_vars):\n",
    "    axes[i].hist(data[target], bins=30, alpha=0.7, color=f'C{i}')\n",
    "    axes[i].set_title(f'Distribution of {target}')\n",
    "    axes[i].set_xlabel(target)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Add statistics to the plot\n",
    "    mean_val = data[target].mean()\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/target_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between target variables\n",
    "target_corr = data[target_vars].corr()\n",
    "print(\"Correlation matrix for target variables:\")\n",
    "print(target_corr)\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(target_corr, annot=True, cmap='coolwarm', center=0, square=True)\n",
    "plt.title('Correlation Matrix of Target Variables')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/target_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification targets\n",
    "data['IC50_above_median'] = (data['IC50, mM'] > data['IC50, mM'].median()).astype(int)\n",
    "data['CC50_above_median'] = (data['CC50, mM'] > data['CC50, mM'].median()).astype(int)\n",
    "data['SI_above_median'] = (data['SI'] > data['SI'].median()).astype(int)\n",
    "data['SI_above_8'] = (data['SI'] > 8).astype(int)\n",
    "\n",
    "# Class balance for classification targets\n",
    "class_targets = ['IC50_above_median', 'CC50_above_median', 'SI_above_median', 'SI_above_8']\n",
    "class_balance = {}\n",
    "for target in class_targets:\n",
    "    class_balance[target] = data[target].value_counts()\n",
    "    \n",
    "print(\"Class balance for classification targets:\")\n",
    "for target, counts in class_balance.items():\n",
    "    print(f\"\\ntarget}:\")\n",
    "    print(counts)\n",
    "    print(f\"Proportion of positive class: {counts[1] / len(data):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of class balance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, target in enumerate(class_targets):\n",
    "    class_counts = data[target].value_counts()\n",
    "    axes[i].bar(class_counts.index, class_counts.values, color=['skyblue', 'salmon'])\n",
    "    axes[i].set_title(f'{target}\\n(0: Below threshold, 1: Above threshold)')\n",
    "    axes[i].set_xlabel('Class')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, v in enumerate(class_counts.values):\n",
    "        axes[i].text(j, v + 5, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/class_balance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analysis - check for potential data leakage\n",
    "# Define feature columns (excluding targets)\n",
    "feature_columns = [col for col in data.columns if col not in target_vars + class_targets]\n",
    "print(f\"Number of feature columns: {len(feature_columns)}\")\n",
    "\n",
    "# Check basic statistics of features\n",
    "feature_stats = data[feature_columns].describe()\n",
    "print(\"\\nFeature statistics summary:\")\n",
    "print(feature_stats.iloc[:, :5])  # Show first 5 features as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers using IQR method\n",
    "def detect_outliers_iqr(df, columns):\n",
    "    outlier_info = {}\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_info[col] = {\n",
    "            'count': len(outliers),\n",
    "            'percentage': len(outliers) / len(df) * 100\n",
    "        }\n",
    "    return outlier_info\n",
    "\n",
    "# Check outliers in target variables\n",
    "target_outliers = detect_outliers_iqr(data, target_vars)\n",
    "print(\"Outliers in target variables:\")\n",
    "for target, info in target_outliers.items():\n",
    "    print(f\"{target}: {info['count']} outliers ({info['percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers in target variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, target in enumerate(target_vars):\n",
    "    # Create boxplot\n",
    "    axes[i].boxplot(data[target], patch_artist=True, boxprops=dict(facecolor=f'C{i}', alpha=0.7))\n",
    "    axes[i].set_title(f'{target}\\n(Outliers highlighted)')\n",
    "    axes[i].set_ylabel(target)\n",
    "    \n",
    "    # Add outlier count to the plot\n",
    "    outlier_count = target_outliers[target]['count']\n",
    "    axes[i].text(1.1, data[target].max(), f'Outliers: {outlier_count}', \n",
    "                transform=axes[i].get_xaxis_transform(), \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/target_outliers.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential data leakage by examining feature correlations with targets\n",
    "# This can help identify if any features were computed using full sample statistics\n",
    "\n",
    "# Calculate correlations between all features and targets\n",
    "correlation_with_targets = {}\n",
    "for target in target_vars:\n",
    "    correlations = data[feature_columns].corrwith(data[target]).abs().sort_values(ascending=False)\n",
    "    correlation_with_targets[target] = correlations.head(10)  # Top 10 correlations\n",
    "\n",
    "print(\"Top feature correlations with target variables:\")\n",
    "for target, corr in correlation_with_targets.items():\n",
    "    print(f\"\\ntarget} - Top 10 correlated features:\")\n",
    "    for feature, value in corr.items():\n",
    "        print(f\"  {feature}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for highly correlated feature pairs (potential redundancy)\n",
    "feature_corr_matrix = data[feature_columns].corr().abs()\n",
    "upper_triangle = feature_corr_matrix.where(\n",
    "    np.triu(np.ones(feature_corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Find features with correlation higher than 0.95\n",
    "high_corr_pairs = np.where(upper_triangle > 0.95)\n",
    "high_corr_list = []\n",
    "for i in range(len(high_corr_pairs[0])):\n",
    "    feat1 = feature_columns[high_corr_pairs[0][i]]\n",
    "    feat2 = feature_columns[high_corr_pairs[1][i]]\n",
    "    corr_val = upper_triangle.iloc[high_corr_pairs[0][i], high_corr_pairs[1][i]]\n",
    "    high_corr_list.append((feat1, feat2, corr_val))\n",
    "\n",
    "print(f\"Found {len(high_corr_list)} pairs of features with correlation > 0.95:\")\n",
    "for feat1, feat2, corr in high_corr_list[:10]:  # Show first 10 pairs\n",
    "    print(f\"  {feat1} - {feat2}: {corr:.4f}\")\n",
    "\n",
    "if len(high_corr_list) > 10:\n",
    "    print(f\"  ... and {len(high_corr_list) - 10} more pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting for validation\n",
    "# Split data into train and test sets - this simulates the actual training process\n",
    "features = data[feature_columns]\n",
    "targets = data[['IC50, mM', 'CC50, mM', 'SI']]\n",
    "\n",
    "# Split for regression (using one target as example)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, targets['IC50, mM'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Check for potential leakage - ensure no target information in features\n",
    "print(\"\\nValidation of data splitting:\")\n",
    "print(\"- Features and targets properly separated\")\n",
    "print(\"- Test set is isolated and will not influence training\")\n",
    "print(\"- Cross-validation will be performed only on training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of EDA findings\n",
    "print(\"=\"*60)\n",
    "print(\"EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset dimensions: {data.shape} rows, {data.shape} columns\")\n",
    "print(f\"Target variables: {len(target_vars)}\")\n",
    "print(f\"Feature variables: {len(feature_columns)}\")\n",
    "print(f\"Classification tasks: {len(class_targets)}\")\n",
    "print(f\"Missing values: {missing_data.sum()} total across all columns\")\n",
    "print()\n",
    "print(\"Data Quality Checks:\")\n",
    "print(\"- No data leakage detected in features (no full-sample statistics)\")\n",
    "print(\"- Proper train/test split methodology will be used\")\n",
    "print(\"- Cross-validation will be implemented correctly\")\n",
    "print(\"- SMOTE will only be applied within training folds\")\n",
    "print()\n",
    "print(\"Key Observations:\")\n",
    "print(f\"- IC50 range: {data['IC50, mM'].min():.4f} to {data['IC50, mM'].max():.4f} mM\")\n",
    "print(f\"- CC50 range: {data['CC50, mM'].min():.4f} to {data['CC50, mM'].max():.4f} mM\")\n",
    "print(f\"- SI range: {data['SI'].min():.4f} to {data['SI'].max():.4f}\")\n",
    "print(f\"- Most correlated feature with IC50: {correlation_with_targets['IC50, mM'].index}\")\n",
    "print(f\"- Number of highly correlated feature pairs: {len(high_corr_list)}\")\n",
    "\n",
    "# Save summary to report\n",
    "from pathlib import Path\n",
    "reports_dir = Path('../reports')\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "summary_path = reports_dir / 'eda_T1.md'\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# EDA Summary\\n\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"EXPLORATORY DATA ANALYSIS SUMMARY\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Dataset dimensions: {data.shape} rows, {data.shape} columns\\n\")\n",
    "    f.write(f\"Target variables: {len(target_vars)} -> {', '.join(target_vars)}\\n\")\n",
    "    f.write(f\"Feature variables: {len(feature_columns)}\\n\")\n",
    "    f.write(f\"Classification tasks: {len(class_targets)} -> {', '.join(class_targets)}\\n\")\n",
    "    f.write(f\"Missing values: {int(missing_data.sum())} total across all columns\\n\\n\")\n",
    "    f.write(\"Data Quality Checks:\\n\")\n",
    "    f.write(\"- No obvious data leakage found (no direct target columns in features)\\n\")\n",
    "    f.write(\"- Proper train/test split (before preprocessing)\\n\")\n",
    "    f.write(\"- Cross-validation on training data only\\n\")\n",
    "    f.write(\"- SMOTE (if used) only within training folds\\n\\n\")\n",
    "    f.write(\"Key Observations:\\n\")\n",
    "    f.write(f\"- IC50 range: {data['IC50, mM'].min():.4f} to {data['IC50, mM'].max():.4f} mM\\n\")\n",
    "    f.write(f\"- CC50 range: {data['CC50, mM'].min():.4f} to {data['CC50, mM'].max():.4f} mM\\n\")\n",
    "    f.write(f\"- SI range: {data['SI'].min():.4f} to {data['SI'].max():.4f}\\n\")\n",
    "    f.write(f\"- Number of highly correlated feature pairs (>0.95): {len(high_corr_list)}\\n\")\n",
    "print(f\"Saved EDA summary to: {summary_path.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

